---
title: "Lab 3-1"
output: 
html_document:
  toc: TRUE
  toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
install.packages("wordcloud")
install.packages("e1071")

```

```{r}
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(e1071)
library(gmodels)

```


##Exploring the Data

```{r}
sms_raw <- read.csv("sms_spam2.csv", stringsAsFactors = FALSE)

```

```{r}
str(sms_raw)

#It looks like the target variable is the first variable, ham or spam, and has two levels. This is the variable we are trying to predict from the second variable, which is our variable containing all the text.

```

```{r}
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)

#The type element is currently a character character vector, but since it is categorical it would be helpful to convert this into a factor.
#Examining the data after changing 'type' into a factor, the transformation has worked. There are 4812 observations of ham and 747 observations of spam.


```

##Cleaning and Standardizing the Data

```{r}
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)

#Creating a corpus from the raw text data in order to process the data better.
#This corpus has 5559 documents.

```

```{r}
inspect(sms_corpus[1:2])

#Inspecting the corpus to view specific messages in our text.

```

```{r}
as.character(sms_corpus[[1]])

#Viewing the actual message from the corpus.

```

```{r}
lapply(sms_corpus[1:2], as.character)

#Viewing multiple documents within corpus.

```

```{r}
sms_corpus_clean <- tm_map(sms_corpus, 
                           content_transformer(tolower))

#Transforming corpus and cleaning the text so that all characters are standardized.

```

```{r}
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])

#Checking to see if the command worked and it did - uppercase letters have been replaced with lowercase letters.

```

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)

#Removing numbers, filler words and punctuation from the clean corpus.

```

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)

#Reducing words to their root form using stemming.

```

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)

#Performing final step of reducing whitespace between words.

```

##Splitting Text Documents into Words

```{r}
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

#Creating a DTM from the clean corpus. The Document Term Matrix function takes a corpus and creates a data structure in which rows indicate documents and columns indicate terms.

sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
  tolower = TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE
))

sms_dtm
sms_dtm2

```

##Creating Training and Test Datasets

```{r}
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]

sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type


#Creating testing and training sets from our data.

prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))

#Visualizing testing and traiing sets.

```

##Word Clouds

```{r}
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)

#Creating a word cloud from the clean corpus to see which words are used most frequently.

```

```{r}
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")

#Creating two data frames containing spam and ham.

```

```{r}
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))

#Visualzing spam and ham word clouds to see which words are most common in each case.

```

##Creating Indicator Features for Frequent Words

```{r}
findFreqTerms(sms_dtm_train, 5)

#This function takes a DTM and and returns a character vector containing the words that appear for at least the specified number of times.

```

```{r}
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)

#This function shows there are 1139 terms appearing the SMS messages.

```

```{r}
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[, sms_freq_words]

#Limiting the DTM to specific words. We want all the rows, but only the columns representing the words in the sms_freq_words vector.

```

```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0 , "Yes", "No")
}

#Since the Naive Bayes classifier is typically trained on data with categorical features, we need to change this to a categorical variable that simply indicates yes or no depending on whether the word appears at all.

```

```{r}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
                   convert_counts)

sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
                  convert_counts)

#We now need to apply the convert_counts to each of the columns in our sparse matrix using the apply function.

```

##Training a Model on the Data

```{r}
sms_classifier <- naiveBayes(sms_train, sms_train_labels)

#Building our model on the sms_train matrix, which can be used to make predictions.

```

##Evaluating Model Performance

```{r}
sms_test_pred <- predict(sms_classifier, sms_test)

```

```{r}
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))

#Creating a cross table from the predicition vector. 2.6% of the messages were classified incorrectly.

```

##Improving Model Performance with the Laplace Estimator

```{r}
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels,
laplace = 1)

sms_test_pred2 <- predict(sms_classifier2, sms_test)

CrossTable(sms_test_pred2, sms_test_labels,
    prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
    dnn = c('predicted', 'actual'))

#Adding the laplace estimator reduced the number of false positives from 6 to 5 and the number of false negatives from 30 to 28.

```

